# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
from IPython import get_ipython

# %% [markdown]
# <a href="https://colab.research.google.com/github/Educat8n/AI-Development-Oxford/blob/main/boston_house_price_prediction.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# %%
get_ipython().run_cell_magic('writefile', 'boston_house_price_prediction.py', '\n\n# Step -1 - Import Packages\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nfrom sklearn import metrics\nplt.rcParams["figure.figsize"] = (10, 10)\n\n\n\n# Step - 2 - Define the main function\ndef main():\n    # Get data\n\n    ### To Do Assignment: try changing the data from Boston housing to California housing dataset \n    ### You can load the datasets as follows::\n    ###    from sklearn.datasets import fetch_california_housing\n    ###    housing = fetch_california_housing()\n    ###  Refer this link for more detatils: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n    boston = load_boston()\n    boston_X = pd.DataFrame(boston.data, columns = boston.feature_names)\n    boston_y = boston.target\n    features = boston.feature_names\n    \n    ## Data Exploration\n    print(f\'The features in dataset are: {features}\')\n    #print(f\'Data description\\n {boston_X.describe()}\')\n    \n    #Plots\n    plot_data(boston_X, boston_y, features, cor=True)\n\n    ## Remove Outliers\n    boston_X, boston_y = remove_outliers(boston_X,boston_y, features)\n    \n    X_train, y_train, X_test, y_test = preprocess(boston_X, boston_y, features)\n\n    model = SVR() \n\n    model = train(model, X_train, y_train)\n\n    evaluate(model, X_test, y_test, bl= True)\n\n    best_params = optimize_models(X_train, y_train)\n    print(best_params)\n\n    ## Build Best Model\n    best_C= best_params[\'C\']\n    best_kernel = best_params[\'kernel\']\n\n    best_model = SVR(kernel = best_kernel, C= best_C)\n    best_model = train(best_model, X_train, y_train)\n    evaluate (best_model, X_test, y_test)\n\n\n    \n\n \n# Step - 3 - Plot graphs to understand data\ndef plot_data(x_df, y_df,features, cor=False):\n    X = x_df.values\n    plt.figure(figsize=(10,10))\n    plt.title("Price Distribution")\n    plt.hist(y_df, bins=30)\n    plt.show()\n    #cols = x_df.columns()\n    fig, ax = plt.subplots(1, len(features), sharey=True, figsize=(20,5))\n    plt.title("Relationship between different input features and price")\n    ax = ax.flatten()\n    for i, col in enumerate(features):\n        x = X[:,i]\n        y = y_df\n        ax[i].scatter(x, y, marker=\'o\')\n        ax[i].set_title(col)\n        ax[i].set_xlabel(col)\n        ax[i].set_ylabel(\'MEDV\')\n    plt.show()\n\n    if cor:\n      pass\n      ### To Do Add the code to find and display correlation among\n      ### different features\n\n\n\n\n# Step - 4 - Preprocess data\n# Step -4a : Remove outliers\ndef remove_outliers(x,y, features):\n    #remove null\n    x_df = x.copy(deep=True)\n    x_df[\'MEDV\'] = y\n    x_df.dropna(inplace=True)\n    return x_df[features], x_df[\'MEDV\']\n    \n    \n# Step -4b : Normalize data\ndef scale_numeric(df):\n    x = df.values \n    scaler = preprocessing.StandardScaler()\n    ### To Do Assignment instead of StandardScaler use MinMaxscaler, \n    ### Also observe if scaling influences the results\n    x_scaled = scaler.fit_transform(x)\n    df = pd.DataFrame(x_scaled)\n    return df\n\n    \n\n# Step -4b : Preprocess data\ndef preprocess(x, y, features):\n    x_df = x[features].copy(deep=True)\n    x_df = scale_numeric(x_df)\n    #print(len(x_df),len(y))\n    # Split data into train, test\n    X_train, X_test, y_train, y_test = train_test_split(x_df,y, test_size=0.3, random_state=1)\n    return X_train, y_train, X_test, y_test\n    \n    \n    \n    \n# Step - 5 - train model \ndef train(model,X_train, y_train):\n    model.fit(X_train, y_train)\n    return model\n    \n    \n# Step - 6 - Evaluate Model\ndef evaluate(model, X_test, y_test, plot = True, print_results=True, bl=False):\n    y_pred = model.predict(X_test)\n    if print_results:\n      if bl:\n        print(\'\\n\\nBaseline Model Performance on Test Dataset:\\n\')\n      else:\n        print(\'\\n\\nBest Model Performance on Test Dataset:\\n\')\n      print(\'R^2:\',metrics.r2_score(y_test, y_pred))\n      print(\'MAE:\',metrics.mean_absolute_error(y_test, y_pred))\n      print(\'MSE:\',metrics.mean_squared_error(y_test, y_pred))\n      print(\'RMSE:\',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\n    if plot:\n      plt.scatter(y_test, y_pred)\n      plt.xlabel("Prices")\n      plt.ylabel("Predicted prices")\n      plt.title("Prices vs Predicted prices")\n      plt.show()\n    return \n    \n    \n    \n    \n# Step - 7 - Improve Model\ndef optimize_models(X_train, y_train):\n  ### To Do Assignment Change the model to MLP  and accordiongly change Grid search params\n  params = {\'kernel\':[\'linear\', \'rbf\'], \'C\':[1, 10]}\n  model = SVR()\n  clf = GridSearchCV(model, params)\n  clf.fit(X_train, y_train)\n  return (clf.best_params_)\n\n\n\n# call the main finction\nif __name__ == \'__main__\':\n    main()\n    \n    \n    ')


# %%
get_ipython().run_line_magic('run', 'boston_house_price_prediction.py')


# %%
get_ipython().run_cell_magic('writefile', 'cancer_detection.py', '\n\n# Step -1 - Import Package\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nfrom sklearn import metrics\nplt.rcParams["figure.figsize"] = (10, 10)\n\n\n\n# Step - 2 - Define the main function\ndef main():\n    # Get data\n    cancer_data = load_breast_cancer()\n    cancer_data_X = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)\n    cancer_data_y = cancer_data.target\n    features = cancer_data.feature_names\n    \n    vars = [\'mean radius\', \'mean texture\', \'mean area\', \'mean perimeter\', \'mean smoothness\']\n    ## Data Exploration\n    print(f\'The features in dataset are: {features}\')\n    #print(f\'Data description\\n {cancer_data_X.describe()}\')\n    \n    #Plots\n    plot_data(cancer_data_X, cancer_data_y, features= vars, cor=True)\n\n    ## Remove Outliers\n    cancer_data_X, cancer_data_y = remove_outliers(cancer_data_X,cancer_data_y, features)\n    \n    X_train, y_train, X_test, y_test = preprocess(cancer_data_X, cancer_data_y, features)\n\n    model = SVC(random_state=6)\n\n    model = train(model, X_train, y_train)\n    \n    baseline = evaluate(model, X_test, y_test, bl=True)\n\n    best_params = optimize_models(X_train, y_train)\n    print(best_params)\n\n    ## Build Best Model\n    best_C= best_params[\'C\']\n    best_kernel = best_params[\'kernel\']\n\n    best_model = SVC(kernel = best_kernel, C= best_C, random_state=6)\n    best_model = train(best_model, X_train, y_train)\n    evaluate (best_model, X_test, y_test)\n    \n\n \n    \n    \n# Step - 3 - Plot graphs to understand data\ndef plot_data(x_df, y_df,features, cor=False):\n    X = x_df.copy(deep=True)\n    X[\'class\'] = y_df\n    sns.pairplot(X, hue = \'class\', vars = [\'mean radius\', \'mean texture\', \'mean area\', \'mean perimeter\', \'mean smoothness\'] )\n    plt.show()\n    \n    if cor:\n      corr = X[features].corr()\n      plt.figure(figsize=(10,10))\n      sns.heatmap(corr, cbar=True, square= True, fmt=\'.1f\', annot=True, annot_kws={\'size\':15}, cmap=\'Greens\')\n      plt.show()\n\n\n\n\n\n\n# Step - 4 - Preprocess data\n# Step -4a : Remove outliers\ndef remove_outliers(x,y, features):\n    #remove null\n    x_df = x.copy(deep=True)\n    x_df[\'class\'] = y\n    x_df.dropna(inplace=True)\n    return x_df[features], x_df[\'class\']\n    \n    \n# Step -4b : Normalize data\ndef scale_numeric(df):\n    x = df.values \n    scaler = preprocessing.StandardScaler()\n    x_scaled = scaler.fit_transform(x)\n    df = pd.DataFrame(x_scaled)\n    return df\n\n    \n\n# Step -4b : Preprocess data\ndef preprocess(x, y, features):\n    x_df = x[features].copy(deep=True)\n    x_df = scale_numeric(x_df)\n    #print(len(x_df),len(y))\n    # Split data into train, test\n    X_train, X_test, y_train, y_test = train_test_split(x_df,y, test_size=0.3, random_state=45)\n    return X_train, y_train, X_test, y_test\n    \n    \n    \n    \n# Step - 5 - train model \ndef train(model,X_train, y_train):\n    model.fit(X_train, y_train)\n    return model\n    \n    \n# Step - 6 - Evaluate Model\ndef evaluate(model, X_test, y_test, plot = True, print_results=True, bl=False):\n    y_pred = model.predict(X_test)\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    acc = metrics.accuracy_score(y_test, y_pred)\n    if print_results:\n      if bl:\n        print(\'\\n\\nBaseline Model Performance on Test Dataset:\\n\')\n      else:\n        print(\'\\n\\nBest Model Performance on Test Dataset:\\n\')\n      print(\'\\nConfusion Matrix:\\n\',cm)\n      print(f\'Accuracy: {acc*100}%\')\n\n    if plot:\n      sns.heatmap(cm, annot= True)\n      plt.show()\n    return \n    \n    \n    \n    \n# Step - 7 - Improve Model\ndef optimize_models(X_train, y_train):\n  params = {\'kernel\':[\'rbf\'], \'C\':[1.0, 5.0, 10]}\n  model = SVC(random_state=5)\n  clf = GridSearchCV(model, params)\n  clf.fit(X_train, y_train)\n  return clf.best_params_\n\n\n# call the main finction\nif __name__ == \'__main__\':\n    main()\n    \n    \n    ')


# %%
get_ipython().run_line_magic('run', 'cancer_detection.py')


# %%



